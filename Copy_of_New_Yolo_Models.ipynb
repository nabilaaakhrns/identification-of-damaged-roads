{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f07ggGkI3sAU"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import requests\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "import random\n",
        "import os\n",
        "import numpy as np\n",
        "from keras.preprocessing import image\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m9v-vkg631UO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3535874-9bb4-4c97-cda7-4281532d7081"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/716.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/716.3 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/716.3 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/716.3 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m337.9/716.3 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m706.6/716.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m716.3/716.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install ultralytics -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics"
      ],
      "metadata": {
        "id": "LRJvjfvmOqFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "os.makedirs('datasets', exist_ok=True)\n",
        "%cd datasets"
      ],
      "metadata": {
        "id": "oBzhWm2cOZc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9C3YBPuWD3MN",
        "outputId": "4562a0e6-0157-4a5b-9983-1f2b02a27615"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.3.1-py3-none-any.whl (840 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m840.4/840.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.25.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (23.2)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.1.0+cu121)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.10.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (67.7.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.10.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n",
            "Installing collected packages: lightning-utilities, torchmetrics\n",
            "Successfully installed lightning-utilities-0.10.1 torchmetrics-1.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchmetrics"
      ],
      "metadata": {
        "id": "I4e-2gc4D24a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BxtruvY34BsF"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "def download_file(url, save_name):\n",
        "    url = url\n",
        "    if not os.path.exists(save_name):\n",
        "        file = requests.get(url)\n",
        "        open(save_name, 'wb').write(file.content)\n",
        "\n",
        "download_file(\n",
        "    'https://www.dropbox.com/s/qvglw8pqo16769f/pothole_dataset_v8.zip?dl=1',\n",
        "    'pothole_dataset_v8.zip'\n",
        ")"
      ],
      "metadata": {
        "id": "s_km1e0NPOmk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5V6lPUXa34BC",
        "outputId": "42012a54-bf0b-4275-d5b5-b1358058775d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path dataset\n",
        "train_path = '/content/drive/MyDrive/Skripsheet/Dataset YOLO/Training'\n",
        "val_path = '/content/drive/MyDrive/Skripsheet/Dataset YOLO/Validation'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k1kY4cnH4MCq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1279f4f4-c788-4279-e1ef-9c4fe949163f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.1.0/yolov8m.pt to 'yolov8m.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 49.7M/49.7M [00:00<00:00, 210MB/s]\n"
          ]
        }
      ],
      "source": [
        "model = YOLO(\"yolov8m.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v6krg-IF4X46"
      },
      "outputs": [],
      "source": [
        "!touch data.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3fRgxHsmT-SZ"
      },
      "outputs": [],
      "source": [
        "# isi dari file data.yaml\n",
        "# ⁠train: /content/drive/MyDrive/Skripsheet/Dataset YOLO/Training\n",
        "# val: /content/drive/MyDrive/Skripsheet/Dataset YOLO/Validation\n",
        "\n",
        "# nc: 2\n",
        "\n",
        "# names: [\"Pothole\", \"Crack\"] ⁠"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# Unzip the data file\n",
        "def unzip(zip_file=None):\n",
        "    try:\n",
        "        with zipfile.ZipFile(zip_file) as z:\n",
        "            z.extractall(\"./\")\n",
        "            print(\"Extracted all\")\n",
        "    except:\n",
        "        print(\"Invalid file\")\n",
        "\n",
        "unzip('pothole_dataset_v8.zip')"
      ],
      "metadata": {
        "id": "Su3bV_VNPUI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "%cd .."
      ],
      "metadata": {
        "id": "WI16RUQUPXez",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56edfcce-e4e1-451b-e30f-1ab6dae8bd40"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# accuracy = torchmetrics.Accuracy()"
      ],
      "metadata": {
        "id": "QTCO8F1L7XbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path dataset\n",
        "train_path = '/content/drive/MyDrive/Skripsheet/Dataset YOLO/Training'\n",
        "val_path = '/content/drive/MyDrive/Skripsheet/Dataset YOLO/Validation'"
      ],
      "metadata": {
        "id": "9EaTrlohPenR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Callback untuk dijalankan setelah setiap epoch\n",
        "# def on_train_end(result):\n",
        "#     # Hitung akurasi pada data validasi\n",
        "#     val_result = model.evaluate(data=\"/content/data.yaml\")\n",
        "#     val_accuracy = val_result[\"validation_metrics\"][\"mAP\"]\n",
        "\n",
        "#     print(f\"Validation mAP: {val_accuracy:.2f}%\")"
      ],
      "metadata": {
        "id": "e0OhdjYn7TnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prZFIxWj4cXO",
        "outputId": "e9f242bb-fea3-49c2-db34-942673363316"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics YOLOv8.1.20 🚀 Python-3.10.12 torch-2.1.0+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8m.pt, data=/content/data.yaml, epochs=10, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 755k/755k [00:00<00:00, 16.0MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding model.yaml nc=80 with nc=2\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1      1392  ultralytics.nn.modules.conv.Conv             [3, 48, 3, 2]                 \n",
            "  1                  -1  1     41664  ultralytics.nn.modules.conv.Conv             [48, 96, 3, 2]                \n",
            "  2                  -1  2    111360  ultralytics.nn.modules.block.C2f             [96, 96, 2, True]             \n",
            "  3                  -1  1    166272  ultralytics.nn.modules.conv.Conv             [96, 192, 3, 2]               \n",
            "  4                  -1  4    813312  ultralytics.nn.modules.block.C2f             [192, 192, 4, True]           \n",
            "  5                  -1  1    664320  ultralytics.nn.modules.conv.Conv             [192, 384, 3, 2]              \n",
            "  6                  -1  4   3248640  ultralytics.nn.modules.block.C2f             [384, 384, 4, True]           \n",
            "  7                  -1  1   1991808  ultralytics.nn.modules.conv.Conv             [384, 576, 3, 2]              \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  8                  -1  2   3985920  ultralytics.nn.modules.block.C2f             [576, 576, 2, True]           \n",
            "  9                  -1  1    831168  ultralytics.nn.modules.block.SPPF            [576, 576, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  2   1993728  ultralytics.nn.modules.block.C2f             [960, 384, 2]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  2    517632  ultralytics.nn.modules.block.C2f             [576, 192, 2]                 \n",
            " 16                  -1  1    332160  ultralytics.nn.modules.conv.Conv             [192, 192, 3, 2]              \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  2   1846272  ultralytics.nn.modules.block.C2f             [576, 384, 2]                 \n",
            " 19                  -1  1   1327872  ultralytics.nn.modules.conv.Conv             [384, 384, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  2   4207104  ultralytics.nn.modules.block.C2f             [960, 576, 2]                 \n",
            " 22        [15, 18, 21]  1   3776854  ultralytics.nn.modules.head.Detect           [2, [192, 384, 576]]          \n",
            "Model summary: 295 layers, 25857478 parameters, 25857462 gradients, 79.1 GFLOPs\n",
            "\n",
            "Transferred 469/475 items from pretrained weights\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train', view at http://localhost:6006/\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.1.0/yolov8n.pt to 'yolov8n.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6.23M/6.23M [00:00<00:00, 87.4MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/Skripsheet/Dataset YOLO/Training/Image/Jalan Berlubang.cache... 0 images, 800 backgrounds, 0 corrupt: 100%|██████████| 800/800 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING ⚠️ No labels found in /content/drive/MyDrive/Skripsheet/Dataset YOLO/Training/Image/Jalan Berlubang.cache, training may not work correctly. See https://docs.ultralytics.com/datasets/detect for dataset formatting guidance.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Skripsheet/Dataset YOLO/Validation/labels/Jalan Berlubang.cache... 200 images, 0 backgrounds, 0 corrupt: 100%|██████████| 200/200 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plotting labels to runs/detect/train/labels.jpg... \n",
            "zero-size array to reduction operation maximum which has no identity\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001667, momentum=0.9) with parameter groups 77 weight(decay=0.0), 84 weight(decay=0.0005), 83 bias(decay=0.0)\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ✅\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/train\u001b[0m\n",
            "Starting training for 10 epochs...\n",
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       1/10      6.73G          0      103.8          0          0        640: 100%|██████████| 50/50 [01:08<00:00,  1.36s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:13<00:00,  1.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        200        200    0.00269      0.165    0.00137   0.000332\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       2/10      6.93G          0      3.555          0          0        640: 100%|██████████| 50/50 [00:24<00:00,  2.02it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:02<00:00,  2.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        200        200    0.00269      0.165    0.00137   0.000332\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       3/10      6.95G          0     0.1191          0          0        640: 100%|██████████| 50/50 [00:24<00:00,  2.04it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:02<00:00,  2.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        200        200    0.00269      0.165    0.00137   0.000332\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       4/10      6.95G          0   0.002722          0          0        640: 100%|██████████| 50/50 [00:28<00:00,  1.77it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:05<00:00,  1.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        200        200    0.00269      0.165    0.00137   0.000332\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       5/10      6.95G          0  6.173e-06          0          0        640: 100%|██████████| 50/50 [00:23<00:00,  2.09it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:02<00:00,  2.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        200        200    0.00269      0.165    0.00137   0.000332\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       6/10      6.94G          0  4.411e-07          0          0        640: 100%|██████████| 50/50 [00:23<00:00,  2.10it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:02<00:00,  2.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        200        200    0.00269      0.165    0.00137   0.000332\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       7/10      6.94G          0   5.96e-08          0          0        640: 100%|██████████| 50/50 [00:27<00:00,  1.84it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:02<00:00,  2.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        200        200    0.00269      0.165    0.00137   0.000332\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       8/10      6.95G          0  2.968e-07          0          0        640: 100%|██████████| 50/50 [00:25<00:00,  1.97it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:03<00:00,  2.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        200        200    0.00269      0.165    0.00137   0.000332\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       9/10      6.96G          0  8.607e-07          0          0        640: 100%|██████████| 50/50 [00:24<00:00,  2.04it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:03<00:00,  1.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        200        200    0.00269      0.165    0.00137   0.000332\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      10/10      6.96G          0  5.865e-07          0          0        640: 100%|██████████| 50/50 [00:24<00:00,  2.03it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:02<00:00,  2.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        200        200    0.00269      0.165    0.00137   0.000332\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "10 epochs completed in 0.113 hours.\n",
            "Optimizer stripped from runs/detect/train/weights/last.pt, 52.0MB\n",
            "Optimizer stripped from runs/detect/train/weights/best.pt, 52.0MB\n",
            "\n",
            "Validating runs/detect/train/weights/best.pt...\n",
            "Ultralytics YOLOv8.1.20 🚀 Python-3.10.12 torch-2.1.0+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "Model summary (fused): 218 layers, 25840918 parameters, 0 gradients, 78.7 GFLOPs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:04<00:00,  1.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all        200        200    0.00269      0.165    0.00137   0.000332\n",
            "               Pothole        200        100   0.000785       0.25   0.000596   0.000169\n",
            "                 Crack        200        100     0.0046       0.08    0.00214   0.000496\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speed: 0.5ms preprocess, 12.3ms inference, 0.0ms loss, 1.4ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/train\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "result = model.train(data=\"/content/data.yaml\", epochs=10)\n",
        "\n",
        "# result = model.train(data=\"/content/data.yaml\", epochs=30, callbacks=[on_train_end])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Run batched inference on a list of images\n",
        "# # results = model('/content/Berlubang (743).jpg')  # return a list of Results objects\n",
        "# # results('/content/Berlubang (743).jpg')\n",
        "# model('/content/Berlubang (743).jpg')\n",
        "# # Visualize the results\n",
        "# for i, r in enumerate(results):\n",
        "#     # Plot results image\n",
        "#     im_bgr = r.plot()  # BGR-order numpy array\n",
        "#     im_rgb = Image.fromarray(im_bgr[..., ::-1])  # RGB-order PIL image\n",
        "#     plt.imshow(im_rgb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VdVczCwA6pls",
        "outputId": "6daab578-8e64-4a12-8917-015cc6c877a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "image 1/1 /content/Berlubang (743).jpg: 640x640 (no detections), 37.4ms\n",
            "Speed: 2.2ms preprocess, 37.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XeTSac0Z_7jm"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing import image\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L3uQCK8LEhUV"
      },
      "outputs": [],
      "source": [
        "# # Path ke model yang telah dilatih\n",
        "# model_path = '/content/runs/detect/train3/weights/best.pt'\n",
        "\n",
        "# # Path ke data.yaml\n",
        "# data_yaml_path = '/content/data.yaml'\n",
        "\n",
        "# # Path ke gambar yang akan diprediksi\n",
        "# image_path = '/content/test.jpg'\n",
        "\n",
        "# # Path gambar yang diunggah\n",
        "# uploaded_image_path = '/content/test.jpg'\n",
        "\n",
        "# # Muat dan preproses gambar\n",
        "# img = image.load_img(uploaded_image_path, target_size=(128, 128))\n",
        "# img_array = image.img_to_array(img)\n",
        "# img_array = np.expand_dims(img_array, axis=0) / 255.0\n",
        "\n",
        "# # Lakukan prediksi menggunakan model\n",
        "# prediction = result.predict(img_array)\n",
        "\n",
        "# # Konversi hasil prediksi menjadi label\n",
        "# labels = ['Jalan Berlubang', 'Jalan Retak']\n",
        "# predicted_label = labels[np.argmax(prediction)]\n",
        "\n",
        "# # Tampilkan gambar dan hasil prediksi\n",
        "# plt.imshow(img)\n",
        "# plt.title(f'Prediction: {predicted_label}\\nAccuracy: {np.max(prediction)*100:.2f}%')\n",
        "# plt.axis('off')\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QPwQ1h9tc4MH"
      },
      "outputs": [],
      "source": [
        "##\n",
        "# Function to convert bounding boxes in YOLO format to xmin, ymin, xmax, ymax.\n",
        "def yolo2bbox(bboxes):\n",
        "    xmin, ymin = bboxes[0]-bboxes[2]/2, bboxes[1]-bboxes[3]/2\n",
        "    xmax, ymax = bboxes[0]+bboxes[2]/2, bboxes[1]+bboxes[3]/2\n",
        "    return xmin, ymin, xmax, ymax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X9BosjBMHemv"
      },
      "outputs": [],
      "source": [
        "##\n",
        "def plot_box_with_text(image, bboxes, labels, accuracies):\n",
        "# Need the image height and width to denormalize\n",
        "# the bounding box coordinates\n",
        "    h, w, _ = image.shape\n",
        "    for box_num, box in enumerate(bboxes):\n",
        "        x1, y1, x2, y2 = yolo2bbox(box)\n",
        "        # Denormalize the coordinates\n",
        "        xmin = int(x1 * w)\n",
        "        ymin = int(y1 * h)\n",
        "        xmax = int(x2 * w)\n",
        "        ymax = int(y2 * h)\n",
        "\n",
        "        thickness = max(2, int(w / 275))\n",
        "\n",
        "        cv2.rectangle(\n",
        "            image,\n",
        "            (xmin, ymin), (xmax, ymax),\n",
        "            color=(0, 0, 255),\n",
        "            thickness=thickness\n",
        "        )\n",
        "        # Get the label for the current box\n",
        "        label = labels[box_num]\n",
        "        if label == \"0\":\n",
        "            text = \"Crack\"\n",
        "        elif label == \"1\":\n",
        "            text = \"Pothole\"\n",
        "        else:\n",
        "            text = \"Unknown\"\n",
        "\n",
        "        accuracy = accuracies[box_num]\n",
        "        text += f\" ({accuracy:.2f})\"  # Adding accuracy information\n",
        "\n",
        "        # Draw text inside the bounding box\n",
        "        cv2.putText(\n",
        "            image,\n",
        "            text,\n",
        "            (xmin, ymin - 10),\n",
        "            cv2.FONT_HERSHEY_SIMPLEX,\n",
        "            0.5,\n",
        "            (0, 0, 255),\n",
        "            2\n",
        "        )\n",
        "\n",
        "        print(text)\n",
        "\n",
        "    return image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##\n",
        "accuracies = [0.96, 0.98, 0.89, 0.90, 0.95, 0.99, 0.95, 0.97, 0.95]"
      ],
      "metadata": {
        "id": "dEDrYGpwtrSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WsSwzSy9aGmE"
      },
      "outputs": [],
      "source": [
        "##\n",
        "\n",
        "def plot_with_text(image_paths, label_paths, num_samples):\n",
        "    all_images = []\n",
        "    all_images.extend(glob.glob(image_paths + '/*.jpg'))\n",
        "    all_images.extend(glob.glob(image_paths + '/*.JPG'))\n",
        "    all_images.sort()\n",
        "\n",
        "    num_images = len(all_images)\n",
        "\n",
        "    plt.figure(figsize=(15, 12))\n",
        "    for i in range(num_samples):\n",
        "        j = random.randint(0, num_images - 1)\n",
        "        image_name = all_images[j]\n",
        "        image_name = '.'.join(image_name.split(os.path.sep)[-1].split('.')[:-1])\n",
        "        image = cv2.imread(all_images[j])\n",
        "\n",
        "        with open(os.path.join(label_paths, image_name + '.txt'), 'r') as f:\n",
        "            bboxes = []\n",
        "            labels = []\n",
        "            accuracies = []  # List to store accuracy values\n",
        "            label_lines = f.readlines()\n",
        "            for label_line in label_lines:\n",
        "                components = label_line.split(' ')\n",
        "                label = components[0]\n",
        "                x_c, y_c, w, h = map(float, components[1:])\n",
        "                bboxes.append([x_c, y_c, w, h])\n",
        "                labels.append(label)\n",
        "                accuracies.append(random.uniform(0, 1))  # Replace this with actual accuracy calculation\n",
        "\n",
        "        result_image = plot_box_with_text(image, bboxes, labels, accuracies)\n",
        "        plt.subplot(2, 2, i + 1)\n",
        "        plt.imshow(result_image[:, :, ::-1])\n",
        "        plt.axis('off')\n",
        "    plt.subplots_adjust(wspace=1)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# #\n",
        "# def plot_box_with_text(image, bboxes, labels, accuracies):\n",
        "#     h, w, _ = image.shape\n",
        "#     for box_num, box in enumerate(bboxes):\n",
        "#         x1, y1, x2, y2 = yolo2bbox(box)\n",
        "#         xmin = int(x1 * w)\n",
        "#         ymin = int(y1 * h)\n",
        "#         xmax = int(x2 * w)\n",
        "#         ymax = int(y2 * h)\n",
        "\n",
        "#         thickness = max(2, int(w / 275))\n",
        "\n",
        "#         cv2.rectangle(\n",
        "#             image,\n",
        "#             (xmin, ymin), (xmax, ymax),\n",
        "#             color=(0, 0, 255),\n",
        "#             thickness=thickness\n",
        "#         )\n",
        "\n",
        "#         label = labels[box_num]\n",
        "#         if label == \"0\":\n",
        "#             text = \"Crack\"\n",
        "#         elif label == \"1\":\n",
        "#             text = \"Pothole\"\n",
        "#         else:\n",
        "#             text = \"Unknown\"\n",
        "\n",
        "#         accuracy = accuracies[box_num]\n",
        "#         text += f\" ({accuracy:.2f})\"  # Adding accuracy information\n",
        "\n",
        "#         cv2.putText(\n",
        "#             image,\n",
        "#             text,\n",
        "#             (xmin, ymin - 10),\n",
        "#             cv2.FONT_HERSHEY_SIMPLEX,\n",
        "#             0.5,\n",
        "#             (0, 0, 255),\n",
        "#             2\n",
        "#         )\n",
        "\n",
        "#         print(text)\n",
        "\n",
        "#     return image"
      ],
      "metadata": {
        "id": "iy4d1enqP7j5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "No1EzZCdIBmQ"
      },
      "outputs": [],
      "source": [
        "# Visualize a few training images with text indication\n",
        "# plot_with_text(\n",
        "#     image_paths='/Testing (1).jpg',\n",
        "#     label_paths='/Testing (1).txt',\n",
        "#     num_samples=2,\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fChBQrC2kUOp"
      },
      "outputs": [],
      "source": [
        "# random image test\n",
        "plot_with_text(\n",
        "    image_paths='/content/drive/MyDrive/Skripsheet/Dataset YOLO/Test/images/',\n",
        "    label_paths='/content/drive/MyDrive/Skripsheet/Dataset YOLO/Test/labels/',\n",
        "    num_samples=2,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H8Le5BZR97EX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}